# Имя задания
name: densenet-train
# Описание задания
desc: densenet-train

# Параметры точки входа для запуска вычислений
cmd: > # многострочная строка YAML
  python3 experiment_densenet_cifar.py \
    --mode train --epochs 200 --name cifar10_densenet_bc_190_40

# Файлы с входными данными
inputs:
  - experiment_densenet_cifar.py
  - densenet_quant.py
  # - /usr/share/params.json: # Абсолютный путь к файлу на локальном компьютере сохранен в переменную PARAMS
  #     var: PARAMS

# Файлы с результатами
outputs:
  - models/
  - results/

# # Ресурсы, необходимые для запуска задания, должны быть доступны в проекте
# s3-mounts: # Коннекторы S3
#   - <идентификатор_коннектора>   # Идентификатор коннектора S3
#                            # Имя коннектора не задано, поэтому обращение к коннектору возможно по его идентификатору
# datasets:
#   - <идентификатор_датасета>:  # Идентификатор датасета, доступного в проекте
#       var: CIFAR   # CIFAR — переменная для обращения к датасету

# Параметры окружения
env:
  # vars: # Переменные окружения
  #   # - MLFLOW_TRACKING_USERNAME: <secret in DataSphere project>
  #   # - MLFLOW_TRACKING_PASSWORD: <secret in DataSphere project>

  # # Способ сборки зависимостей окружения
  # python: auto # Полная автоматизация сборки окружения

  python: # Параметры окружения задаются вручную. Если параметры не заданы, их значения будут определены из текущего окружения автоматически
    type: manual
    version: 3.12.2 # Версия Python
    pip:
      index-url: https://pypi.org/simple # Адрес основного репозитория для установки пакетов
      # extra-index-urls: # Адреса дополнительных репозиториев
      #   - https://pypi.ngc.nvidia.com
      # trusted-hosts: # Список доверенных хостов
      #   - nvidia.com
      # no-deps: true  # По умолчанию false
    requirements-file: requirements.txt # Файл с параметрами окружения
    root-path: # Явное указание дополнительных точек входа
      - experiment_densenet_cifar.py
    local-paths: # Список локальных Python-файлов, которые нужно перенести. Нельзя использовать с опцией root-paths
      - densenet_quant.py

# Флаги запуска задания
flags:
  - attach-project-disk # Смонтировать хранилище проекта

# Конфигурации вычислительных ресурсов для запуска задания
cloud-instance-types:
  - g2.1 # A100

# # Конфигурация расширенной рабочей директории
# working-storage:
#   type: SSD    # тип используемого диска. Опционально, по умолчанию SSD. Доступные значения: SSD
#   size: 150Gb  # размер рабочей директории в интервале 100 ГБ — 10 ТБ

# Конфигурация плавного завершения работы
graceful-shutdown:
  signal:
    SIGTERM # Сигнал, который будет отправлен процессу задания при нажатии Ctrl + C (cancel), по умолчанию SIGTERM
    # Доступные значения: SIGTERM, SIGINT, SIGHUP, SIGUSR1, SIGUSR2
  timeout: 15s # Таймаут, через который процесс задания получит SIGKILL, если не успевает завершиться

# # Список датасетов, которые будут созданы при успешном завершении задания
# output-datasets:
#   - name: job-test-dataset-1  # Название датасета
#     var: OUT_DS               # Переменная, содержащая путь до датасета. Содержимое указанной директории будет оформлено в виде датасета
#     description: "Описание"
#     size: 100Gb               # Максимальный объем данных в датасете
#     labels:                   # Произвольный список меток, которые будут присвоены датасету
#       a: b
#       c: d

# # Настройка подключеня к кластеру DataProc через Spark Connector
# spark:
#   connector: <идентификатор_коннектора> # Идентификатор Spark Connector
